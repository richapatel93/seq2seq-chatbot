# Seq2Seq Chatbot (Student Learning Project)

This project is based on code by **Dhruvil Shah**, originally published in [this article](https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-5e057ccd1f3e).  
I followed the tutorial to understand how generative chatbots work using the Seq2Seq architecture in Keras.

---

## 🙋‍♀️ Why I Did This

I'm learning about NLP and deep learning, and this was my first hands-on project building a chatbot using LSTM-based encoder-decoder models. While the model didn’t generate great replies due to limited data, I learned how Seq2Seq models work under the hood.

---

## 📸 Chatbot Conversation Examples

| **User Input**       | **Chatbot Response** |
|----------------------|----------------------|
| how are you?         | i i i i i            |
| what is your name?   | i i i i i i          |
| tell me a joke?      | i i i i              |

Yes — it needs improvement 😅 but the learning was great!

---

## 🧠 What I Learned

- What encoder-decoder models are and how they pass information
- How to tokenize, vectorize, and pad text data
- How to test a generative chatbot in Python using Keras
- Common issues in training with small datasets (like repeated outputs)

---

## 📁 What’s in This Repo

- `chatbot.ipynb`: The notebook I ran and explored
- `screenshots/`: My actual chatbot results
- `README.md`: My reflections and project explanation

---

## 🔧 Future Improvements

If I continue this project, I’d:
- Train on a larger dataset (e.g., Cornell Movie Dialogues)
- Add attention mechanism
- Use pre-trained word embeddings (like GloVe)
- Try transformer-based models (e.g., T5, BART)

---

## 📚 Credit

Original code and article by [Dhruvil Shah](https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-5e057ccd1f3e)

---

### 🧑‍💻 This repo is part of my learning journey in NLP. I'm open to feedback and suggestions!

